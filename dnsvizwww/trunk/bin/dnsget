#!/usr/bin/env python

import codecs
import collections
import datetime
import getopt
import json
import logging
import sys
import multiprocessing
import multiprocessing.managers
import threading

import dns.name

from dnsviz.analysis import get_client_addresses, NetworkConnectivityException, _resolver
import dnsviz.format as fmt
from dnsvizwww.models import Analyst, DomainNameAnalysis

logger = logging.getLogger('dnsviz.analysis')

#XXX this is a hack required for inter-process sharing of dns.name.Name
# instances using multiprocess
def _setattr_dummy(self, name, value):
    return super(dns.name.Name, self).__setattr__(name, value)
dns.name.Name.__setattr__ = _setattr_dummy

def _analyze((cls, name, dlv_domain, client_ipv4, client_ipv6, force_ancestry, start_time)):
    try:
        a = cls(name, dlv_domain=dlv_domain, client_ipv4=client_ipv4, client_ipv6=client_ipv6, start_time=start_time, force_ancestry=force_ancestry)
        return a.analyze()
    except:
        logger.exception('Error analyzing %s' % name.canonicalize().to_text())

class BulkAnalyst(object):
    def __init__(self, force_ancestry, dlv_domain):
        self.client_ipv4, self.client_ipv6 = get_client_addresses()
        if self.client_ipv4 is None and self.client_ipv6 is None:
            raise NetworkConnectivityException('No network interfaces available for analysis!')
        self.force_ancestry = force_ancestry
        self.dlv_domain = dlv_domain

        self.start_time = datetime.datetime.now(fmt.utc).replace(microsecond=0)

    def analyze(self, names):
        name_objs = []
        for name in names:
            name_objs.append(_analyze((Analyst, name, self.dlv_domain, self.client_ipv4, self.client_ipv6, self.force_ancestry, self.start_time)))
        return name_objs

class MultiProcessAnalyst(Analyst):
    analysis_model = DomainNameAnalysis

class ParallelAnalyst(BulkAnalyst):
    def __init__(self, force_ancestry, dlv_domain, processes):
        super(ParallelAnalyst, self).__init__(force_ancestry, dlv_domain)
        self.manager = multiprocessing.managers.SyncManager()
        self.manager.start()

        self.processes = processes

        self.start_time = datetime.datetime.now(fmt.utc).replace(microsecond=0)

    def analyze(self, names):
        pool = multiprocessing.Pool(self.processes)

        def _name_to_args_iter(names):
            for name in names:
                yield (MultiProcessAnalyst, name, self.dlv_domain, self.client_ipv4, self.client_ipv6, self.force_ancestry, self.start_time)

        return pool.map(_analyze, _name_to_args_iter(names))

def usage():
    sys.stderr.write('''Usage: %s [ options ] ( -f <filename> | <domain name> [... ] )

Options:
    -f <filename>  - read names from a file (one name per line), instead of from command line
    -d <level>     - set debug level to a value from 0 to 3, with increasing verbosity (default: 1 or WARNING)
    -l <dlv>       - use dlv as a domain for DNSSEC look-aside validation
    -r <filename>  - read analysis from a file, instead of querying servers (use "-" for stdin)
    -t <threads>   - use multiple threads for analysis
    -F             - force analysis of ancestry, instead of relying on cached versions
    -p             - make json output pretty instead of minimal
    -y             - read in yaml, instead of json
    -Y             - write in yaml, instead of json
    -o <filename>  - write the analysis to file instead of to stdout
''' % sys.argv[0])

def main():
    try:
        opts, args = getopt.getopt(sys.argv[1:], 'f:d:l:r:t:pyYo:F')
    except getopt.GetoptError:
        usage()
        sys.exit(1)

    opts = dict(opts)
    if ('-f' in opts and args) or not ('-f' in opts or args):
        usage()
        sys.exit(1)

    if '-l' in opts:
        dlv_domain = dns.name.from_text(opts['-l'])
    else:
        dlv_domain = None

    force_ancestry = '-F' in opts

    try:
        processes = int(opts.get('-t', 1))
    except ValueError:
        usage()
        sys.exit(1)
    if processes < 1:
        usage()
        sys.exit(1)

    try:
        val = int(opts.get('-d', 1))
    except ValueError:
        usage()
        sys.exit(1)

    if val < 0 or val > 3:
        usage()
        sys.exit(1)

    if val > 2:
        debug_level = logging.DEBUG
    elif val > 1:
        debug_level = logging.INFO
    elif val > 0:
        debug_level = logging.WARNING
    else:
        debug_level = logging.ERROR
    handler = logging.StreamHandler()
    handler.setLevel(debug_level)
    logger.addHandler(handler)
    logger.setLevel(debug_level)

    if '-f' in opts:
        names = []
        with codecs.open(opts['-f'], 'r', 'utf-8') as f:
            for line in f:
                names.append(dns.name.from_unicode(line.strip()))
    else:
        names = map(dns.name.from_text, args)

    name_objs = []
    if '-r' in opts:
        if opts['-r'] == '-':
            analysis_str = sys.stdin.read()
        else:
            analysis_str = open(opts['-r']).read()
        if '-y' in opts:
            import yaml
            analysis_structured = yaml.load(analysis_str)
        else:
            analysis_structured = json.loads(analysis_str)
        for name in names:
            name_objs.append(DomainNameAnalysis.deserialize(name, analysis_structured))
    else:
        if '-t' in opts:
            a = ParallelAnalyst(force_ancestry, dlv_domain, processes)
        else:
            a = BulkAnalyst(force_ancestry, dlv_domain)
        name_objs = a.analyze(names)

    sys.exit(0)

    d = collections.OrderedDict()
    for name_obj in name_objs:
        if name_obj is None:
            continue
        name_obj.serialize(d)

    if '-p' in opts:
        kwargs = { 'indent': 4, 'separators': (',', ': ') }
    else:
        kwargs = {}

    if '-o' not in opts or opts['-o'] == '-':
        fh = sys.stdout
    else:
        fh = open(opts['-o'], 'w')

    if '-Y' in opts:
        import yaml
        fh.write(yaml.dump(d))
    else:
        fh.write(json.dumps(d, **kwargs))

if __name__ == "__main__":
    main()
