#!/usr/bin/env python

import codecs
import collections
import datetime
import getopt
import json
import logging
import sys
import multiprocessing
import multiprocessing.managers
import threading
import time

import dns.name

from django.conf import settings

from dnsvizwww.analysis import Analyst, DomainNameAnalysis
from dnsvizwww.models import DomainName
from dnsviz.analysis import get_client_addresses, NetworkConnectivityException, _resolver
import dnsviz.format as fmt

logger = logging.getLogger('dnsviz.analysis')

#XXX this is a hack required for inter-process sharing of dns.name.Name
# instances using multiprocess
def _setattr_dummy(self, name, value):
    return super(dns.name.Name, self).__setattr__(name, value)
dns.name.Name.__setattr__ = _setattr_dummy

def _analyze((cls, name, dlv_domain, client_ipv4, client_ipv6, force_ancestry, start_time)):
    try:
        a = cls(name, dlv_domain=dlv_domain, client_ipv4=client_ipv4, client_ipv6=client_ipv6, start_time=start_time, force_ancestry=force_ancestry)
        return a.analyze()
    except KeyboardInterrupt:
        pass
    except:
        logger.exception('Error analyzing %s' % name.canonicalize().to_text())

class BulkAnalyst(object):
    analyst_cls = Analyst

    def __init__(self, force_ancestry, dlv_domain):
        self.client_ipv4, self.client_ipv6 = get_client_addresses()
        if self.client_ipv4 is None and self.client_ipv6 is None:
            raise NetworkConnectivityException('No network interfaces available for analysis!')
        self.force_ancestry = force_ancestry
        self.dlv_domain = dlv_domain

        self.start_time = datetime.datetime.now(fmt.utc).replace(microsecond=0)

    def _name_to_args_iter(self, names):
        for name in names:
            yield (self.analyst_cls, name, self.dlv_domain, self.client_ipv4, self.client_ipv6, self.force_ancestry, self.start_time)

    def analyze(self, names):
        for args in self._name_to_args_iter(names):
            _analyze(args)

class MultiProcessAnalyst(Analyst):
    analysis_model = DomainNameAnalysis

class ParallelAnalyst(BulkAnalyst):
    analyst_cls = MultiProcessAnalyst

    def __init__(self, force_ancestry, dlv_domain, processes):
        super(ParallelAnalyst, self).__init__(force_ancestry, dlv_domain)
        self.manager = multiprocessing.managers.SyncManager()
        self.manager.start()

        self.processes = processes

        self.start_time = datetime.datetime.now(fmt.utc).replace(microsecond=0)

    def analyze(self, names):

        pool = multiprocessing.Pool(self.processes, maxtasksperchild=50)
        try:
            for args in self._name_to_args_iter(names):
                while pool._taskqueue.full():
                    time.sleep(0.5)
                pool.apply_async(_analyze, (args,))
        except KeyboardInterrupt:
            pass

        pool.close()
        try:
            pool.join()
        except KeyboardInterrupt:
            pool.terminate()
        pool.join()

    def refresh(self):
        wait_time = 20
        last_refresh_offsets = {}

        last_stats = 0
        stats_interval = 30
        refreshed = 0

        pool = multiprocessing.Pool(self.processes, maxtasksperchild=5)
        try:
            while True:
                refresh_intervals = set(DomainName.objects.filter(refresh_interval__isnull=False).values_list('refresh_interval', flat=True).distinct())

                # synchronize refresh_intervals and last_refresh_offsets
                for i in set(last_refresh_offsets).union(refresh_intervals):
                    if i not in last_refresh_offsets:
                        last_refresh_offsets[i] = None
                    if i not in refresh_intervals:
                        del last_refresh_offsets[i]

                # at the start of every loop check for names being analyzed
                start = int(time.time())
                timestamp = datetime.datetime.now(fmt.utc).replace(microsecond=0)
                tot = 0
                for interval, last_offset in last_refresh_offsets.items():
                    offset = DomainName.objects.offset_for_interval(interval)
                    if last_offset is not None:
                        names_to_refresh = set(map(dns.name.from_text, DomainName.objects.names_to_refresh(interval, offset, last_offset).values_list('name', flat=True)))

                        for args in self._name_to_args_iter(names_to_refresh):
                            while pool._taskqueue.full():
                                time.sleep(0.5)
                            pool.apply_async(_analyze, (args,))

                        refreshed += len(names_to_refresh)
                    last_refresh_offsets[interval] = offset

                end = int(time.time())
                elapsed = end - start
                if elapsed < wait_time:
                    time.sleep(wait_time - elapsed)
                time_since_stats = end - last_stats
                if time_since_stats >= stats_interval:
                    now = datetime.datetime.now(fmt.utc).replace(microsecond=0)
                    last_stats = end
                    logger.warning('%s: (refreshed: %d; taskqueue: %d) ' % (now, refreshed, pool._taskqueue.qsize()))
                    refreshed = 0

        except KeyboardInterrupt:
            pass

        pool.close()
        try:
            pool.join()
        except KeyboardInterrupt:
            pool.terminate()
        pool.join()

def usage():
    sys.stderr.write('''Usage: %s [ options ] ( -R | -f <filename> | <domain name> [... ] )

Options:
    -f <filename>  - read names from a file (one name per line), instead of from command line
    -d <level>     - set debug level to a value from 0 to 3, with increasing verbosity (default: 2 or INFO)
    -l <dlv>       - use dlv as a domain for DNSSEC look-aside validation
    -r <filename>  - read analysis from a file, instead of querying servers (use "-" for stdin)
    -R             - refresh names on a periodic basis
    -t <threads>   - use multiple threads for analysis
    -F             - force analysis of ancestry, instead of relying on cached versions
    -p             - make json output pretty instead of minimal
    -y             - read in yaml, instead of json
    -Y             - write in yaml, instead of json
    -o <filename>  - write the analysis to file instead of to stdout
''' % sys.argv[0])

def main():
    try:
        opts, args = getopt.getopt(sys.argv[1:], 'f:d:l:r:Rt:pyYo:F')
    except getopt.GetoptError:
        usage()
        sys.exit(1)

    opts = dict(opts)
    if ('-f' in opts and args) or ('-R' in opts and args) or ('-f' in opts and '-R' in opts) or \
            not ('-f' in opts or '-R' in opts or args):
        usage()
        sys.exit(1)

    if '-l' in opts:
        dlv_domain = dns.name.from_text(opts['-l'])
    else:
        dlv_domain = None

    force_ancestry = '-F' in opts

    try:
        processes = int(opts.get('-t', 1))
    except ValueError:
        usage()
        sys.exit(1)
    if processes < 1:
        usage()
        sys.exit(1)

    try:
        val = int(opts.get('-d', 2))
    except ValueError:
        usage()
        sys.exit(1)

    if val < 0 or val > 3:
        usage()
        sys.exit(1)

    if val > 2:
        debug_level = logging.DEBUG
    elif val > 1:
        debug_level = logging.INFO
    elif val > 0:
        debug_level = logging.WARNING
    else:
        debug_level = logging.ERROR
    has_handler = False
    # check if there's already a StreamHandler that allows messages through the
    # filters by default
    for handler in logger.handlers:
        if isinstance(handler, logging.StreamHandler):
            if False not in [f.filter(None) for f in handler.filters]:
                has_handler = True
                break
    if not has_handler:
        handler = logging.StreamHandler()
        handler.setLevel(debug_level)
        logger.addHandler(handler)
    logger.setLevel(debug_level)

    if '-f' in opts:
        names = []
        with codecs.open(opts['-f'], 'r', 'utf-8') as f:
            for line in f:
                try:
                    name = dns.name.from_unicode(line.strip())
                except UnicodeDecodeError, e:
                    logger.warning('%s: %s' % (line.strip(), e))
                    continue

                names.append(name)
    else:
        names = map(dns.name.from_text, args)

    name_objs = []
    if '-r' in opts:
        if opts['-r'] == '-':
            analysis_str = sys.stdin.read()
        else:
            analysis_str = open(opts['-r']).read()
        if '-y' in opts:
            import yaml
            analysis_structured = yaml.load(analysis_str)
        else:
            analysis_structured = json.loads(analysis_str)
        for name in names:
            name_objs.append(DomainNameAnalysis.deserialize(name, analysis_structured))
    elif '-R' in opts:
        a = ParallelAnalyst(force_ancestry, dlv_domain, processes)
        a.refresh()
    else:
        if '-t' in opts:
            a = ParallelAnalyst(force_ancestry, dlv_domain, processes)
        else:
            a = BulkAnalyst(force_ancestry, dlv_domain)
        name_objs = a.analyze(names)

    sys.exit(0)

    d = collections.OrderedDict()
    for name_obj in name_objs:
        if name_obj is None:
            continue
        name_obj.serialize(d)

    if '-p' in opts:
        kwargs = { 'indent': 4, 'separators': (',', ': ') }
    else:
        kwargs = {}

    if '-o' not in opts or opts['-o'] == '-':
        fh = sys.stdout
    else:
        fh = open(opts['-o'], 'w')

    if '-Y' in opts:
        import yaml
        fh.write(yaml.dump(d))
    else:
        fh.write(json.dumps(d, **kwargs))

if __name__ == "__main__":
    main()
